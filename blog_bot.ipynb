{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-generate travel blogs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Example script to generate blogging text from nomadic matt's blog.\n",
    "\n",
    "Starting off by training a simple model with very little text\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content of all the travel blogs have been crawled and output to a text file. Load the ASCII text of the travel blogs into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download over a network\n",
    "# path = get_file('output.txt', origin='https://www......')\n",
    "# with io.open(path, encoding='utf-8') as f:\n",
    "#     text = f.read().lower()\n",
    "\n",
    "text = ''\n",
    "with open(\"blog.txt\", 'r') as content_file:\n",
    "    for line in content_file.readlines():\n",
    "        text += line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total raw text chars: 6654892\n",
      "total vocab: 155\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¬°', '¬£', '¬Æ', '¬∞', '¬±', '¬¥', '¬º', '¬Ω', '√Å', '√Ö', '√á', '√â', '√ç', '√é', '√ó', '√ü', '√†', '√°', '√¢', '√£', '√§', '√•', '√¶', '√ß', '√®', '√©', '√™', '√¨', '√≠', '√Ø', '√∞', '√±', '√≤', '√≥', '√¥', '√∂', '√∏', '√∫', '√ª', '√º', '√Ω', 'ƒÅ', 'ƒ´', 'ƒ±', '≈ü', '≈°', 'ÃÅ', '‚Äì', '‚Äî', '‚Äò', '‚Äô', '‚Äú', '‚Äù', '‚Ä¶', '‚Ä≥', '‚Ç¨', '‚úì', 'üòâ', 'üôÅ', 'üôÇ']\n"
     ]
    }
   ],
   "source": [
    "def print_stats(text, chars):\n",
    "    print('Total raw text chars:', len(text))    \n",
    "    print('total vocab:', len(chars))\n",
    "    print(chars)\n",
    "    \n",
    "chars = sorted(list(set(text)))\n",
    "print_stats(text, chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total vocab for this text comes to about 155 unique characters. Most of them are not required and can be removed. We shall cleanup the text to retain only ASCII characters and convert all alphabets to lowercase, which reduces the text size and brings down the vocab count from 155 to 69."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total raw text chars: 6614263\n",
      "total vocab: 69\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "def get_ASCII_text(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+','', text)\n",
    "\n",
    "text = get_ASCII_text(text).lower()\n",
    "chars = sorted(list(set(text)))\n",
    "print_stats(text, chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there are some more characters that we could remove to further clean up the dataset that will reduce the vocabulary and may improve the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total raw text chars: 6510374\n",
      "total vocab: 58\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '(', ')', '*', '+', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|']\n"
     ]
    }
   ],
   "source": [
    "chars_to_remove = ['&', '[', ']', '\\\\', '^', '_', '`', '{', '}', '~', \"'\", '/']\n",
    "\n",
    "for ch in chars_to_remove:\n",
    "    if ch in text:\n",
    "        text = text.replace(ch, '')\n",
    "        \n",
    "chars = sorted(list(set(text)))\n",
    "print_stats(text, chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare data for modelling by the neural network, we cannot model the characters directly. We should first convert the characters to integers. This can be done by mapping every character to an integer. \n",
    "\n",
    "Also, when preparing the mapping of unique characters to integers, we must also create a reverse mapping that we can use to convert the integers back to characters so that we can understand the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to define the training data for the network. There is a lot of flexibility in how we choose to break up the text and expose it to the network during training.\n",
    "We shall split the blog text into subsequences of 40 characters. We shall slide this window of 100 characters along the entire blog text, jumping 3 characters at every stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 2170112\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network. Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default. In order to do that, we shall one-hot encode the input sequences by creating a 3-dimensional matrix representation of sentences:\n",
    "* The 1st dimension is the total of all sentences (nb sequences).\n",
    "* The 2nd dimension is the length of each sentence, in our case 40\n",
    "* The 3rd dimension is the length of total vocab/unique characters, in our case 59.\n",
    "\n",
    "The output consists of a one-hot encoded 2-dimensional matrix. \n",
    "* The 1st dimension remains the same as input.\n",
    "* The 2nd dimension has the length of total vocab/unique characters.\n",
    "\n",
    "The output pair tells the next char for every input pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Total patterns 2170112\n",
      "Input vector shape (2170112, 40, 58)\n",
      "Output vector shape (2170112, 58)\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "print(\"Total patterns\", len(x))\n",
    "print(\"Input vector shape\", x.shape)\n",
    "print(\"Output vector shape\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = K.pow(2.0, cross_entropy)\n",
    "    return perplexity\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[perplexity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a model checkpoint\n",
    "filepath=\"weights-improvement.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text with an LSTM Network\n",
    "\n",
    "The simplest way to use the Keras LSTM model to make predictions is to first start off with a seed sequence as input, generate the next character then update the seed sequence to add the generated character on the end and trim off the first character. This process is repeated for as long as we want to predict new characters (e.g. a sequence of 400 characters in length).\n",
    "\n",
    "We can pick a random input pattern as our seed sequence, then print generated characters as we generate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2169984/2170112 [============================>.] - ETA: 0s - loss: 1.5983 - perplexity: 15683.8785\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"red while i was there. however, even thr\"\n",
      "red while i was there. however, even three and the street and a be and the many of the read and i had to be a complete in the post to the signific and have to be a street and in a comment of the stranger and the street and i had to the time of an internation and when i had the stranger and the time to the travel than a coupling and the way to be the streets of the beach to the world of the street of the side of the one of the country an\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"red while i was there. however, even thr\"\n",
      "red while i was there. however, even three way, the end to travel, i was than i had had a shoulder and day of the reading a feel that and it was a need to save an example down one of the decided to be completely met to save to be neights and and had a make the board what i constantly be such as as that offer during how if i immediately stopped for the history and more as when i want i want to be a cring to learned that that must confide\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"red while i was there. however, even thr\"\n",
      "red while i was there. however, even threeering old certainly back and a steve to crigned out list.for not and processorced in soon and simply accomemorihesped after near over themeals was sealing, soy, but innerantw too be lost in nights it easlsents meachleffecks having my face. as that braining about so, gimt pallje rainter despite insteadalf sood whole three working as you had to such nocionas..i accomplen my minutes was and larger \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"red while i was there. however, even thr\"\n",
      "red while i was there. however, even through floeves, street that i, the mannew for pienginglegerable, i find in anight forthour and resul more extermanavoraging to sukh at somewhat theres hand every tif oney.hall gijenk.waspera sread of driver discovered almost take dont transyubmiblys.constantlylaute me with their lish ception in any efferents, sote than mydmentce blown, at a good oclit appess andmyshi drink worth (anachassorew than m\n",
      "Epoch 00000: loss improved from inf to 1.59833, saving model to weights-improvement.hdf5\n",
      "2170112/2170112 [==============================] - 1414s - loss: 1.5983 - perplexity: 15683.7448  \n"
     ]
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "history = model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=1, \n",
    "          shuffle=False,\n",
    "          callbacks=[print_callback, checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
